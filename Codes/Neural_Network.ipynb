{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import tweepy\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_unit_id</th>\n",
       "      <th>_golden</th>\n",
       "      <th>_unit_state</th>\n",
       "      <th>_trusted_judgments</th>\n",
       "      <th>_last_judgment_at</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment:confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason:confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>681448150</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>2/25/15 5:24</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2/24/15 11:35</td>\n",
       "      <td>5.703060e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>681448153</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>2/25/15 1:53</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2/24/15 11:15</td>\n",
       "      <td>5.703010e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>681448156</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>2/25/15 10:01</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2/24/15 11:15</td>\n",
       "      <td>5.703010e+17</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>681448158</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>2/25/15 3:05</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2/24/15 11:15</td>\n",
       "      <td>5.703010e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>681448159</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>2/25/15 5:50</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2/24/15 11:14</td>\n",
       "      <td>5.703010e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    _unit_id  _golden _unit_state  _trusted_judgments _last_judgment_at  \\\n",
       "0  681448150    False   finalized                   3      2/25/15 5:24   \n",
       "1  681448153    False   finalized                   3      2/25/15 1:53   \n",
       "2  681448156    False   finalized                   3     2/25/15 10:01   \n",
       "3  681448158    False   finalized                   3      2/25/15 3:05   \n",
       "4  681448159    False   finalized                   3      2/25/15 5:50   \n",
       "\n",
       "  airline_sentiment  airline_sentiment:confidence negativereason  \\\n",
       "0           neutral                        1.0000            NaN   \n",
       "1          positive                        0.3486            NaN   \n",
       "2           neutral                        0.6837            NaN   \n",
       "3          negative                        1.0000     Bad Flight   \n",
       "4          negative                        1.0000     Can't Tell   \n",
       "\n",
       "   negativereason:confidence         airline airline_sentiment_gold  \\\n",
       "0                        NaN  Virgin America                    NaN   \n",
       "1                     0.0000  Virgin America                    NaN   \n",
       "2                        NaN  Virgin America                    NaN   \n",
       "3                     0.7033  Virgin America                    NaN   \n",
       "4                     1.0000  Virgin America                    NaN   \n",
       "\n",
       "         name negativereason_gold  retweet_count  \\\n",
       "0     cairdin                 NaN              0   \n",
       "1    jnardino                 NaN              0   \n",
       "2  yvonnalynn                 NaN              0   \n",
       "3    jnardino                 NaN              0   \n",
       "4    jnardino                 NaN              0   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0                @VirginAmerica What @dhepburn said.         NaN   \n",
       "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
       "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
       "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
       "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
       "\n",
       "   tweet_created      tweet_id tweet_location               user_timezone  \n",
       "0  2/24/15 11:35  5.703060e+17            NaN  Eastern Time (US & Canada)  \n",
       "1  2/24/15 11:15  5.703010e+17            NaN  Pacific Time (US & Canada)  \n",
       "2  2/24/15 11:15  5.703010e+17      Lets Play  Central Time (US & Canada)  \n",
       "3  2/24/15 11:15  5.703010e+17            NaN  Pacific Time (US & Canada)  \n",
       "4  2/24/15 11:14  5.703010e+17            NaN  Pacific Time (US & Canada)  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airlines_csv = '../Resources/Airline-Sentiment-2-w-AA.csv'\n",
    "airlines_df = pd.read_csv(airlines_csv, encoding = 'ISO-8859-1')\n",
    "airlines_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neutral' 'positive' 'neutral' ... 'neutral' 'negative' 'neutral']\n"
     ]
    }
   ],
   "source": [
    "features = airlines_df.iloc[:, 14].values\n",
    "features\n",
    "labels = airlines_df.iloc[:, 5].values\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaned the data by chaning to lowercase\n",
    "\n",
    "clean_data = []                 \n",
    "for feature in features:\n",
    "        item = ' '.join(word.lower() for word in feature.split() \\\n",
    "            if not word.startswith('#') and \\\n",
    "            not word.startswith('@') and \\\n",
    "            not word.startswith('http') and \\\n",
    "            not word.startswith('RT'))\n",
    "        \n",
    "        if item == \"\" or item == \"RT\":\n",
    "                continue\n",
    "        clean_data.append(item)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what said',\n",
       " 'plus youve added commercials to the experience tacky',\n",
       " 'i didnt today must mean i need to take another trip',\n",
       " 'its really aggressive to blast obnoxious entertainment in your guests faces amp they have little recourse',\n",
       " 'and its a really big bad thing about it',\n",
       " 'seriously would pay 30 a flight for seats that didnt have this playing its really the only bad thing about flying va',\n",
       " 'yes nearly every time i fly vx this ear worm wont go away',\n",
       " 'really missed a prime opportunity for men without hats parody there',\n",
       " 'well i didntbut now i do d',\n",
       " 'it was amazing and arrived an hour early youre too good to me']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://www.earthdatascience.org/courses/earth-analytics-python/using-apis-natural-language-processing-twitter/calculate-tweet-word-frequencies-in-python/\n",
    "# used code from link above to remove all special characters \n",
    "def remove_url(txt):\n",
    "    return \" \".join(re.sub(\"([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \"\", txt).split())\n",
    "\n",
    "cleaned_data_no_urls = [remove_url(tweet) for tweet in clean_data]\n",
    "cleaned_data_no_urls[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the labels to numbers\n",
    "sentiments = ['positive', 'negative', 'neutral']\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(sentiments)\n",
    "list(le.classes_)\n",
    "train_labels = le.transform(labels) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create pipeline using Multinomial naive_bayes\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MLPClassifier()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set parameters to test\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {\n",
    "#number of combined words for tokenization\n",
    "'vect__ngram_range': [(1, 1), (1, 2), (1,3)],\n",
    "#remove words above a specified threshold (used in place of stop words)\n",
    "'vect__max_df': (0.25, 0.5, 0.75, 1.0),\n",
    "#include idf\n",
    "'tfidf__use_idf': (True, False),\n",
    "#smoothing parameter\n",
    "'clf__hidden_layer_sizes': [(100,), (200,), (300,)],\n",
    "'clf__activation': ('identity','logistic','tanh','relu'),\n",
    "'clf__solver': ('lbfgs', 'sgd', 'adam'),\n",
    "'clf__alpha':(0.0001, 0.001, 0.00001)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create gridsearch model\n",
    "#gs_clf = GridSearchCV(text_clf, parameters, cv=5, iid=False)\n",
    "gs_clf = GridSearchCV(text_clf, parameters, cv=5, iid=False, n_jobs = -1, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2592 candidates, totalling 12960 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "predictor = gs_clf.fit(cleaned_data_no_urls, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Best Score: {gs_clf.best_score_}')                              \n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print((param_name, gs_clf.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "dump(predictor, 'Models/NeuralMLPClassifier.joblib')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
